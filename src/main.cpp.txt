#include "ml/csv.h"
#include "ml/linreg.h"
#include "ml/poly.h"
#include "ml/plot.h"

#include <iostream>
#include <filesystem>
#include <random>
#include <chrono>
#include <algorithm>
#include <fstream>

// ---------- helpers (local to main.cpp) ----------
static void train_val_split(const ml::Mat &X, const ml::VecN &y,
                            double val_ratio,
                            ml::Mat &Xtr, ml::VecN &ytr,
                            ml::Mat &Xva, ml::VecN &yva,
                            unsigned seed = 42)
{
    const size_t n = X.size();
    std::vector<size_t> idx(n);
    for (size_t i = 0; i < n; i++)
        idx[i] = i;

    std::mt19937 rng(seed);
    std::shuffle(idx.begin(), idx.end(), rng);

    size_t n_val = static_cast<size_t>(n * val_ratio);
    if (n_val < 1)
        n_val = 1;
    if (n_val >= n)
        n_val = n - 1;

    Xva.clear();
    yva.clear();
    Xtr.clear();
    ytr.clear();
    Xva.reserve(n_val);
    yva.reserve(n_val);
    Xtr.reserve(n - n_val);
    ytr.reserve(n - n_val);

    for (size_t k = 0; k < n; k++)
    {
        size_t i = idx[k];
        if (k < n_val)
        {
            Xva.push_back(X[i]);
            yva.push_back(y[i]);
        }
        else
        {
            Xtr.push_back(X[i]);
            ytr.push_back(y[i]);
        }
    }
}

// Build regression dataset: y = "area" and X = all other numeric cols
static void build_Xy_area(const ml::Mat &numeric_all, ml::Mat &X, ml::VecN &y)
{
    if (numeric_all.empty())
        throw std::runtime_error("Empty dataset");
    size_t d = numeric_all[0].size();
    if (d < 2)
        throw std::runtime_error("Need >=2 numeric columns");

    X.assign(numeric_all.size(), ml::VecN(d - 1));
    y.assign(numeric_all.size(), 0.0);

    for (size_t i = 0; i < numeric_all.size(); i++)
    {
        for (size_t j = 0; j < d - 1; j++)
            X[i][j] = numeric_all[i][j];
        y[i] = numeric_all[i][d - 1]; // last numeric column = area
    }
}

// Evaluate a model given weights
static double eval_mse(const ml::Mat &X, const ml::VecN &y, const ml::VecN &w)
{
    return ml::mse(X, y, w);
}

int main()
{
    try
    {
        // ------------- Create results directory -------------
        std::filesystem::create_directories("results");

        // ------------- Load data (forestfire) -------------
        // Header indices: X(0),Y(1),month(2),day(3),...,area(12)
        // Skip month/day => all remaining become numeric columns.
        std::vector<std::string> kept_headers;
        ml::Mat all_numeric = ml::load_numeric_matrix("datasets/forestfires.csv", {2, 3}, &kept_headers);

        // Build X,y for regression demo: predict area
        ml::Mat X_raw;
        ml::VecN y;
        build_Xy_area(all_numeric, X_raw, y);

        // Train/val split
        ml::Mat Xtr_raw, Xva_raw;
        ml::VecN ytr, yva;
        train_val_split(X_raw, y, 0.2, Xtr_raw, ytr, Xva_raw, yva, 42);

        std::cout << "\n================ Linear Regression Comparison ================\n";

        // Add bias
        ml::Mat Xtr_lin = ml::add_bias(Xtr_raw);
        ml::Mat Xva_lin = ml::add_bias(Xva_raw);

        // ---------------- Normal Equation ----------------
        auto t1 = std::chrono::high_resolution_clock::now();
        ml::VecN w_ne = ml::normal_equation(Xtr_lin, ytr);
        auto t2 = std::chrono::high_resolution_clock::now();

        double time_ne = std::chrono::duration<double>(t2 - t1).count();

        // ---------------- Gradient Descent ----------------
        const double alpha = 1e-6;
        const int iters = 3000;

        auto t3 = std::chrono::high_resolution_clock::now();
        ml::VecN w_gd = ml::gradient_descent(Xtr_lin, ytr, alpha, iters);
        auto t4 = std::chrono::high_resolution_clock::now();

        double time_gd = std::chrono::duration<double>(t4 - t3).count();

        // ---------------- MSE Evaluation ----------------
        double train_mse_ne = ml::mse(Xtr_lin, ytr, w_ne);
        double val_mse_ne = ml::mse(Xva_lin, yva, w_ne);

        double train_mse_gd = ml::mse(Xtr_lin, ytr, w_gd);
        double val_mse_gd = ml::mse(Xva_lin, yva, w_gd);

        // ---------------- Print Results ----------------
        std::cout << "\nNormal Equation:\n";
        std::cout << "Train MSE: " << train_mse_ne << "\n";
        std::cout << "Val   MSE: " << val_mse_ne << "\n";
        std::cout << "Time (sec): " << time_ne << "\n";

        std::cout << "\nGradient Descent:\n";
        std::cout << "Train MSE: " << train_mse_gd << "\n";
        std::cout << "Val   MSE: " << val_mse_gd << "\n";
        std::cout << "Time (sec): " << time_gd << "\n";

        // ---------------- Weight Comparison ----------------
        std::cout << "\nWeight Difference (L2 norm): ";
        double diff = 0.0;
        for (size_t i = 0; i < w_ne.size(); i++)
        {
            double d = w_ne[i] - w_gd[i];
            diff += d * d;
        }
        std::cout << std::sqrt(diff) << "\n";

        // ============================================
        // Plot 1: Train vs Val MSE vs polynomial degree
        // ============================================
        const int max_degree = 6;

        std::vector<double> degrees, train_mse_deg, val_mse_deg;
        for (int deg = 1; deg <= max_degree; deg++)
        {
            // Expand features
            ml::Mat Xtr_poly = ml::poly_expand(Xtr_raw, deg);
            ml::Mat Xva_poly = ml::poly_expand(Xva_raw, deg);

            // Add bias
            Xtr_poly = ml::add_bias(Xtr_poly);
            Xva_poly = ml::add_bias(Xva_poly);

            // Fit using normal equation (unregularized)
            ml::VecN w = ml::normal_equation(Xtr_poly, ytr);

            degrees.push_back((double)deg);
            train_mse_deg.push_back(eval_mse(Xtr_poly, ytr, w));
            val_mse_deg.push_back(eval_mse(Xva_poly, yva, w));
        }

        ml::write_two_series_csv("results/mse_vs_degree.csv",
                                 degrees, train_mse_deg, val_mse_deg,
                                 "degree", "train_mse", "val_mse");

        ml::write_gnuplot_script_two_series_png("results/mse_vs_degree.gp",
                                                "results/mse_vs_degree.csv",
                                                "results/mse_vs_degree.png",
                                                "Train vs Validation MSE vs Polynomial Degree",
                                                "Polynomial Degree", "MSE",
                                                "Train", "Validation");

        // ============================================
        // Plot 2: Train vs Val MSE vs lambda (L2 ridge)
        // ============================================
        // Fix degree to something that can overfit a bit (e.g., 4)
        const int ridge_degree = 4;

        // Candidate lambdas (log-spaced style)
        std::vector<double> lambdas = {0.0, 1e-6, 1e-4, 1e-2, 1e-1, 1.0, 10.0};

        std::vector<double> train_mse_lam, val_mse_lam;

        ml::Mat Xtr_r = ml::add_bias(ml::poly_expand(Xtr_raw, ridge_degree));
        ml::Mat Xva_r = ml::add_bias(ml::poly_expand(Xva_raw, ridge_degree));

        for (double lam : lambdas)
        {
            ml::VecN w = ml::ridge_normal_equation(Xtr_r, ytr, lam);

            train_mse_lam.push_back(eval_mse(Xtr_r, ytr, w));
            val_mse_lam.push_back(eval_mse(Xva_r, yva, w));
        }

        ml::write_two_series_csv("results/mse_vs_lambda.csv",
                                 lambdas, train_mse_lam, val_mse_lam,
                                 "lambda", "train_mse", "val_mse");

        ml::write_gnuplot_script_two_series_png("results/mse_vs_lambda.gp",
                                                "results/mse_vs_lambda.csv",
                                                "results/mse_vs_lambda.png",
                                                "Train vs Validation MSE vs L2 Regularization (Ridge)",
                                                "Lambda", "MSE",
                                                "Train", "Validation");

        // ============================================
        // Plot 3: GD convergence (loss vs iterations)
        // ============================================
        // Use degree 1 (linear) for a stable GD demo
        ml::Mat Xtr_gd = ml::add_bias(Xtr_raw);

        const int gd_iters_log = 2000;
        const double gd_alpha_log = 1e-6; // may need tuning if features are unscaled

        ml::VecN w(Xtr_gd[0].size(), 0.0);
        std::vector<double> it_x, loss_y;
        it_x.reserve(gd_iters_log);
        loss_y.reserve(gd_iters_log);

        // Do GD manually here so we can log loss every iteration
        for (int t = 0; t < gd_iters_log; t++)
        {
            // loss
            double L = ml::mse(Xtr_gd, ytr, w);
            it_x.push_back((double)t);
            loss_y.push_back(L);

            // gradient
            const size_t n = Xtr_gd.size();
            const size_t d = Xtr_gd[0].size();
            ml::VecN yhat = ml::matvec(Xtr_gd, w);

            ml::VecN grad(d, 0.0);
            for (size_t j = 0; j < d; j++)
            {
                double s = 0.0;
                for (size_t i = 0; i < n; i++)
                {
                    s += Xtr_gd[i][j] * (yhat[i] - ytr[i]);
                }
                grad[j] = (2.0 / (double)n) * s;
            }

            for (size_t j = 0; j < d; j++)
                w[j] -= gd_alpha_log * grad[j];
        }

        // ---------------- Save ALL results to CSV ----------------
        {
            // 1) Linear regression comparison (NE vs GD)
            std::ofstream comp("results/linear_comparison.csv");
            comp << "method,train_mse,val_mse,time_sec\n";
            comp << "normal equation," << train_mse_ne << "," << val_mse_ne << "," << time_ne << "\n";
            comp << "gradient descent," << train_mse_gd << "," << val_mse_gd << "," << time_gd << "\n";
        }

        // 2) Degree sweep (train vs val)
        ml::write_two_series_csv("results/mse_vs_degree.csv",
                                 degrees, train_mse_deg, val_mse_deg,
                                 "degree", "train_mse", "val_mse");

        // 3) Lambda sweep (train vs val)
        ml::write_two_series_csv("results/mse_vs_lambda.csv",
                                 lambdas, train_mse_lam, val_mse_lam,
                                 "lambda", "train_mse", "val_mse");

        // 4) GD loss curve
        ml::write_xy_csv("results/gd_loss.csv", it_x, loss_y, "iter", "train_mse");

        // ---------------- Generate ALL gnuplot scripts ----------------
        ml::write_gnuplot_script_two_series_png("results/mse_vs_degree.gp",
                                                "results/mse_vs_degree.csv",
                                                "results/mse_vs_degree.png",
                                                "Train vs Validation MSE vs Polynomial Degree",
                                                "Polynomial Degree", "MSE",
                                                "Train", "Validation");

        ml::write_gnuplot_script_two_series_png("results/mse_vs_lambda.gp",
                                                "results/mse_vs_lambda.csv",
                                                "results/mse_vs_lambda.png",
                                                "Train vs Validation MSE vs L2 Regularization (Ridge)",
                                                "Lambda", "MSE",
                                                "Train", "Validation");

        ml::write_gnuplot_script_xy_png("results/gd_loss.gp",
                                        "results/gd_loss.csv",
                                        "results/gd_loss.png",
                                        "Gradient Descent Convergence (Train MSE vs Iterations)",
                                        "Iteration", "Train MSE");

        // ---------------- Plot comparison (NE vs GD) ----------------

        // Plot 1: Train MSE bars
        ml::write_gnuplot_script_bars_png("results/linear_train_mse.gp",
                                          "results/linear_comparison.csv",
                                          "results/linear_train_mse.png",
                                          "Linear Regression: Train MSE (Normal Eq vs GD)",
                                          "Train MSE",
                                          2, "Train MSE");

        ml::run_gnuplot("results/linear_train_mse.gp");

        // Plot 2: Validation MSE bars
        ml::write_gnuplot_script_bars_png("results/linear_val_mse.gp",
                                          "results/linear_comparison.csv",
                                          "results/linear_val_mse.png",
                                          "Linear Regression: Validation MSE (Normal Eq vs GD)",
                                          "Validation MSE",
                                          3, "Validation MSE");

        ml::run_gnuplot("results/linear_val_mse.gp");

        // Plot 3: Time bars
        ml::write_gnuplot_script_bars_png("results/linear_time.gp",
                                          "results/linear_comparison.csv",
                                          "results/linear_time.png",
                                          "Linear Regression: Runtime (Normal Eq vs GD)",
                                          "Time (seconds)",
                                          4, "Runtime (sec)");

        ml::run_gnuplot("results/linear_time.gp");

        // ---------------- Run gnuplot (optional) ----------------
        // bool ok1 = ml::run_gnuplot("results/mse_vs_degree.gp");
        // bool ok2 = ml::run_gnuplot("results/mse_vs_lambda.gp");
        // bool ok3 = ml::run_gnuplot("results/gd_loss.gp");

        // ---------------- Print summary ----------------
        std::cout << "\n================ Results Summary ================\n";
        std::cout << "CSV files:\n";
        std::cout << "  results/linear_comparison.csv\n";
        std::cout << "  results/mse_vs_degree.csv\n";
        std::cout << "  results/mse_vs_lambda.csv\n";
        std::cout << "  results/gd_loss.csv\n";

        std::cout << "\nGnuplot scripts:\n";
        std::cout << "  results/mse_vs_degree.gp\n";
        std::cout << "  results/mse_vs_lambda.gp\n";
        std::cout << "  results/gd_loss.gp\n";

        std::cout << "\nPNG plots:\n";
        std::cout << "  results/mse_vs_degree.png\n";
        std::cout << "  results/mse_vs_lambda.png \n";
        std::cout << "  results/gd_loss.png\n";

        std::cout << "\nComparison plots:\n";
        std::cout << "  results/linear_train_mse.png\n";
        std::cout << "  results/linear_val_mse.png\n";
        std::cout << "  results/linear_time.png\n";

        std::cout << "=================================================\n\n";

        return 0;
    }
    catch (const std::exception &e)
    {
        std::cerr << "Error: " << e.what() << "\n";
        return 1;
    }
}
